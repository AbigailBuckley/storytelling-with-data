{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fruits = ['banana', 'apple',  'mango']\n",
    "# for index in range(len(fruits)):\n",
    "#    print 'Current fruit :', fruits[index]\n",
    "\n",
    "# print \"Good bye!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import argparse\n",
    "# import re\n",
    "# from multiprocessing.pool import ThreadPool as Pool\n",
    "# import requests\n",
    "# import bs4\n",
    "\n",
    "# root_url = 'http://pyvideo.org'\n",
    "# index_url = root_url + '/category/50/pycon-us-2014'\n",
    "\n",
    "\n",
    "# def get_video_page_urls():\n",
    "#     response = requests.get(index_url)\n",
    "#     soup = bs4.BeautifulSoup(response.text)\n",
    "#     return [a.attrs.get('href') for a in soup.select('div.video-summary-data a[href^=/video]')]\n",
    "\n",
    "\n",
    "# def get_video_data(video_page_url):\n",
    "#     video_data = {}\n",
    "#     response = requests.get(root_url + video_page_url)\n",
    "#     soup = bs4.BeautifulSoup(response.text)\n",
    "#     video_data['title'] = soup.select('div#videobox h3')[0].get_text()\n",
    "#     video_data['speakers'] = [a.get_text() for a in soup.select('div#sidebar a[href^=/speaker]')]\n",
    "\n",
    "#     # initialize counters\n",
    "#     video_data['views'] = 0\n",
    "#     video_data['likes'] = 0\n",
    "#     video_data['dislikes'] = 0\n",
    "\n",
    "#     try:\n",
    "#         video_data['youtube_url'] = soup.select('div#sidebar a[href^=http://www.youtube.com]')[0].get_text()\n",
    "#         response = requests.get(video_data['youtube_url'], headers={'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36'})\n",
    "#         soup = bs4.BeautifulSoup(response.text)\n",
    "#         video_data['views'] = int(re.sub('[^0-9]', '',\n",
    "#                                          soup.select('.watch-view-count')[0].get_text().split()[0]))\n",
    "#         video_data['likes'] = int(re.sub('[^0-9]', '',\n",
    "#                                          soup.select('#watch-like-dislike-buttons span.yt-uix-button-content')[0].get_text().split()[0]))\n",
    "#         video_data['dislikes'] = int(re.sub('[^0-9]', '',\n",
    "#                                             soup.select('#watch-like-dislike-buttons span.yt-uix-button-content')[2].get_text().split()[0]))\n",
    "#     except:\n",
    "#         # some or all of the counters could not be scraped\n",
    "#         pass\n",
    "#     return video_data\n",
    "\n",
    "\n",
    "# def parse_args():\n",
    "#     parser = argparse.ArgumentParser(description='Show PyCon 2014 video statistics.')\n",
    "#     parser.add_argument('--sort', metavar='FIELD', choices=['views', 'likes', 'dislikes'],\n",
    "#                         default='views',\n",
    "#                         help='sort by the specified field. Options are views, likes and dislikes.')\n",
    "#     parser.add_argument('--max', metavar='MAX', type=int, help='show the top MAX entries only.')\n",
    "#     parser.add_argument('--csv', action='store_true', default=False,\n",
    "#                         help='output the data in CSV format.')\n",
    "#     parser.add_argument('--workers', type=int, default=8,\n",
    "#                         help='number of workers to use, 8 by default.')\n",
    "#     return parser.parse_args()\n",
    "\n",
    "# def show_video_stats(options):\n",
    "#     video_page_urls = get_video_page_urls()\n",
    "#     for video_page_url in video_page_urls:\n",
    "#        print get_video_data(video_page_url)\n",
    "#     pool = Pool(options.workers)\n",
    "#     video_page_urls = get_video_page_urls()\n",
    "#     results = sorted(pool.map(get_video_data, video_page_urls), key=lambda video: video[options.sort],\n",
    "#                      reverse=True)\n",
    "#     print len(results)\n",
    "#     max = options.max\n",
    "#     if max is None or max > len(results):\n",
    "#         max = len(results)\n",
    "#     if options.csv:\n",
    "#         print(u'\"title\",\"speakers\", \"views\",\"likes\",\"dislikes\"')\n",
    "#     else:\n",
    "#         print(u'Views  +1  -1 Title (Speakers)')\n",
    "#     for i in range(max):\n",
    "#         if options.csv:\n",
    "#             print(u'\"{0}\",\"{1}\",{2},{3},{4}'.format(\n",
    "#                 results[i]['title'], ', '.join(results[i]['speakers']), results[i]['views'],\n",
    "#                 results[i]['likes'], results[i]['dislikes']))\n",
    "#         else:\n",
    "#             print(u'{0:5d} {1:3d} {2:3d} {3} ({4})'.format(\n",
    "#                 results[i]['views'], results[i]['likes'], results[i]['dislikes'], results[i]['title'],\n",
    "#                 ', '.join(results[i]['speakers'])))\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     show_video_stats(parse_args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# youtube url scraper -- Classy @ HF\n",
    "# requirements: python 2.7.X and Requests\n",
    "\n",
    "\n",
    "def get_ch_html(url):\n",
    "    req = requests.get(url)\n",
    "    return req.text\n",
    "\n",
    "\n",
    "def scrape(url, channel):\n",
    "    index = 1\n",
    "    found = 0\n",
    "    retlist = []\n",
    "    print (\"[!] Scraping..\\n\")\n",
    "\n",
    "    while 1:\n",
    "        videoshtml = get_ch_html(url % (channel, index))\n",
    "        #re_pattern = \"\\/watch\\?(?=.*v=\\w+)(?:\\S+)\\\"\"\n",
    "        #Youtube says video is unavailable\n",
    "        re_pattern = \"a href=\\\"http://www.youtube.com/watch\\?v=(.*?)&amp;\"\n",
    "        reg = re.compile(re_pattern)\n",
    "        m = reg.findall(videoshtml)\n",
    "        found += len(m)\n",
    "        retlist.extend(m)\n",
    "        if len(m) == 0:\n",
    "            print (\"[+] Scraping ended.\")\n",
    "            break\n",
    "        index += 50\n",
    "    print (\"[!] Videos found:\", found)\n",
    "        \n",
    "    return retlist\n",
    "\n",
    "def modurls(scrapelist):\n",
    "    retlist = []\n",
    "    for x in scrapelist:\n",
    "        #Takes you to the youtube homepage\n",
    "        retlist.append(\"http://youtube.com/watch?v=\" + x)\n",
    "    return retlist\n",
    "\n",
    "def filesave(scrapelist):\n",
    "    filename = input(\"Enter filename to save as (including .txt): \")\n",
    "    while os.path.isfile(filename):\n",
    "        print (\"File already exists! We don't want to overwrite it!\")\n",
    "        filename = input(\"Enter a different filename: \")\n",
    "\n",
    "    if filename == \"\":\n",
    "        filename = \"videolist.txt\"\n",
    "    f = open(filename, 'w')\n",
    "\n",
    "    f.write(\"Videos found: %d\\n\" % int(len(scrapelist)))\n",
    "    for line in scrapelist:\n",
    "        f.write(line + '\\r\\n')\n",
    "    f.close()\n",
    "\n",
    "def new():\n",
    "    print (\"Youtube URL scraper by Classy @ HF\")\n",
    "    channel =input(\"Channel name: \")\n",
    "    #This link is bad\n",
    "    url = \"https://www.youtube.com/watch?v=4YfNaB2AnCw\"\n",
    "    urls = scrape(url, channel)\n",
    "    urls = modurls(urls)\n",
    "    filesave(urls)\n",
    "\n",
    "    print (\"Looks like scraper executed successfully!\")\n",
    "    print (\"Terminating..\")\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scraped_page = scrape(https://www.youtube.com/watch?v=4YfNaB2AnCw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import hypertools as hyp\n",
    "# import numpy as np\n",
    "# from textblob import TextBlob as tb\n",
    "# import nltk\n",
    "# import datetime as dt\n",
    "# nltk.download('brown')\n",
    "# nltk.download('punkt')\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### START BOILERPLATE CODE\n",
    "\n",
    "# Sample Python code for user authorization\n",
    "\n",
    "# import httplib2\n",
    "# import os\n",
    "# import sys\n",
    "\n",
    "# from apiclient.discovery import build\n",
    "# from apiclient.errors import HttpError\n",
    "# from oauth2client.client import flow_from_clientsecrets\n",
    "# from oauth2client.file import Storage\n",
    "# from oauth2client.tools import argparser, run_flow\n",
    "\n",
    "# # The CLIENT_SECRETS_FILE variable specifies the name of a file that contains\n",
    "# # the OAuth 2.0 information for this application, including its client_id and\n",
    "# # client_secret.\n",
    "# CLIENT_SECRETS_FILE = \"client_secrets.json\"\n",
    "\n",
    "# # This OAuth 2.0 access scope allows for full read/write access to the\n",
    "# # authenticated user's account and requires requests to use an SSL connection.\n",
    "# YOUTUBE_READ_WRITE_SSL_SCOPE = \"https://www.googleapis.com/auth/youtube.force-ssl\"\n",
    "# API_SERVICE_NAME = \"youtube\"\n",
    "# API_VERSION = \"v3\"\n",
    "\n",
    "# # This variable defines a message to display if the CLIENT_SECRETS_FILE is\n",
    "# # missing.\n",
    "# MISSING_CLIENT_SECRETS_MESSAGE = \"WARNING: Please configure OAuth 2.0\" \n",
    "\n",
    "# # Authorize the request and store authorization credentials.\n",
    "# def get_authenticated_service(args):\n",
    "#   flow = flow_from_clientsecrets(CLIENT_SECRETS_FILE, scope=YOUTUBE_READ_WRITE_SSL_SCOPE,\n",
    "#     message=MISSING_CLIENT_SECRETS_MESSAGE)\n",
    "\n",
    "#   storage = Storage(\"youtube-api-snippets-oauth2.json\")\n",
    "#   credentials = storage.get()\n",
    "\n",
    "#   if credentials is None or credentials.invalid:\n",
    "#     credentials = run_flow(flow, storage, args)\n",
    "\n",
    "#   # Trusted testers can download this discovery document from the developers page\n",
    "#   # and it should be in the same directory with the code.\n",
    "#   return build(API_SERVICE_NAME, API_VERSION,\n",
    "#       http=credentials.authorize(httplib2.Http()))\n",
    "\n",
    "\n",
    "# args = argparser.parse_args()\n",
    "# service = get_authenticated_service(args)\n",
    "\n",
    "# def print_results(results):\n",
    "#   print(results)\n",
    "\n",
    "# # Build a resource based on a list of properties given as key-value pairs.\n",
    "# # Leave properties with empty values out of the inserted resource.\n",
    "# def build_resource(properties):\n",
    "#   resource = {}\n",
    "#   for p in properties:\n",
    "#     # Given a key like \"snippet.title\", split into \"snippet\" and \"title\", where\n",
    "#     # \"snippet\" will be an object and \"title\" will be a property in that object.\n",
    "#     prop_array = p.split('.')\n",
    "#     ref = resource\n",
    "#     for pa in range(0, len(prop_array)):\n",
    "#       is_array = False\n",
    "#       key = prop_array[pa]\n",
    "#       # Convert a name like \"snippet.tags[]\" to snippet.tags, but handle\n",
    "#       # the value as an array.\n",
    "#       if key[-2:] == '[]':\n",
    "#         key = key[0:len(key)-2:]\n",
    "#         is_array = True\n",
    "#       if pa == (len(prop_array) - 1):\n",
    "#         # Leave properties without values out of inserted resource.\n",
    "#         if properties[p]:\n",
    "#           if is_array:\n",
    "#             ref[key] = properties[p].split(',')\n",
    "#           else:\n",
    "#             ref[key] = properties[p]\n",
    "#       elif key not in ref:\n",
    "#         # For example, the property is \"snippet.title\", but the resource does\n",
    "#         # not yet have a \"snippet\" object. Create the snippet object here.\n",
    "#         # Setting \"ref = ref[key]\" means that in the next time through the\n",
    "#         # \"for pa in range ...\" loop, we will be setting a property in the\n",
    "#         # resource's \"snippet\" object.\n",
    "#         ref[key] = {}\n",
    "#         ref = ref[key]\n",
    "#       else:\n",
    "#         # For example, the property is \"snippet.description\", and the resource\n",
    "#         # already has a \"snippet\" object.\n",
    "#         ref = ref[key]\n",
    "#   return resource\n",
    "\n",
    "# # Remove keyword arguments that are not set\n",
    "# def remove_empty_kwargs(**kwargs):\n",
    "#   good_kwargs = {}\n",
    "#   if kwargs is not None:\n",
    "#     for key, value in kwargs.iteritems():\n",
    "#       if value:\n",
    "#         good_kwargs[key] = value\n",
    "#   return good_kwargs\n",
    "\n",
    "# ### END BOILERPLATE CODE\n",
    "\n",
    "# # Sample python code for comments.list\n",
    "\n",
    "# def comments_list(service, **kwargs):\n",
    "#   kwargs = remove_empty_kwargs(**kwargs) # See full sample for function\n",
    "#   results = service.comments().list(\n",
    "#     **kwargs\n",
    "#   ).execute()\n",
    "\n",
    "#   print_results(results)\n",
    "\n",
    "# comments_list(service,\n",
    "#     part='snippet',\n",
    "#     parentId='z13icrq45mzjfvkpv04ce54gbnjgvroojf0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get comments from Instagram\n",
    "#Get comments from YouTube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loop through the comments on Instagram to find the most common words used \n",
    "#Loop through the comments on YouTube to find the most common words used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Analyze the sentiment of the comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#I will pick one hip hop video and one ballet video and find the most common words used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#I will get comments from other videos without looking at them first and try to predict what style \n",
    "#of dance the video is based on the words used in the comments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
