{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/optnfs/el7/jupyterhub/envs/Psych81.09/lib/python3.6/site-packages/hypertools/plot/__init__.py:10: UserWarning: Could not switch backend to TkAgg.  This may impact performance of the plotting functions.\n",
      "  warnings.warn('Could not switch backend to TkAgg.  This may impact performance of the plotting functions.')\n"
     ]
    }
   ],
   "source": [
    "#Imported relevant and necessary libraries and data cleaning tools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import hypertools as hyp\n",
    "from glob import glob as lsdir\n",
    "import os\n",
    "import re\n",
    "import datetime as dt\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code from Professor Manning to set up and read in the relevant UVLT data \n",
    "data_readers = {'xlsx': pd.read_excel, 'xls': pd.read_excel, 'dta': pd.read_stata}\n",
    "get_extension = lambda x: x.split('.')[-1]\n",
    "\n",
    "\n",
    "def read_data(datadir, readers):\n",
    "    files = lsdir(os.path.join(datadir, '*'))\n",
    "    readable_files = []\n",
    "    data = []\n",
    "    for f in files:\n",
    "        ext = get_extension(f)\n",
    "        if ext in readers.keys():\n",
    "            readable_files.append(f)\n",
    "            data.append(data_readers[ext](f))\n",
    "    return readable_files, data\n",
    "\n",
    "\n",
    "fnames, data = read_data('data', data_readers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A summary of the data files that are now read into the notebook\n",
    "fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-65c055c17099>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#This is the individual data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#Most of the initial cleaning that I have done has been on this data set, though obviously it can be applied if necessary to any of the data sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#This is the individual data\n",
    "#Most of the initial cleaning that I have done has been on this data set, though obviously it can be applied if necessary to any of the data sets\n",
    "data[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renaming relevant columns in UVLT individual data to be more easily readable\n",
    "names={'DeceasedDateYN' : 'Is the donor Deceased?',\n",
    "       'U_Tot_Amt': 'Total Unrestricted Donations',\n",
    "      'U_Tot_Cnt': 'Total Number of Unrestricted Donations Given',\n",
    "      'ConservedOwner' : 'Owns Conserved Land?',\n",
    "      'RTotAmt' : 'Total Restricted Donations',\n",
    "       'RTotCnt': 'Total Number of Restricted Donations Given',\n",
    "      'VTotCnt' : 'Total Volunteer Occurances',\n",
    "      'ETotCnt' : 'Total Event Attendances'}\n",
    "data[0].rename(names, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary of the column values in data set 1\n",
    "data[0].columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copying each set of data into more memorably named versions\n",
    "#I figured different analyses require different aspects of each dataframe, so starting here and using copies of the data for different analyses may be helpful for organizationa and fidelity\n",
    "Individual_data=data[0].copy()\n",
    "Final_data=data[1].copy()\n",
    "Mailing_data=data[2].copy()\n",
    "Town_Data=data[5].copy()\n",
    "\n",
    "\n",
    "#Similarly to the individual data, the final data could benefit from some cleaner column names\n",
    "Final_data.rename(names, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop={'U200001',\n",
    "        'U200102',\n",
    "        'U200203',\n",
    "        'U200304',\n",
    "        'U200405',\n",
    "        'U200506',\n",
    "        'U200607',\n",
    "        'U200708',\n",
    "        'U200809',\n",
    "        'U200910',\n",
    "        'U201011',\n",
    "        'U201112',\n",
    "        'U201213',\n",
    "        'U201314',\n",
    "        'U201415',\n",
    "        'U201516',\n",
    "        'U201617',\n",
    "        'U201718',\n",
    "        'U201819',\n",
    "        'R200001', \n",
    "        'R200102',\n",
    "        'R200203', \n",
    "        'R200304',\n",
    "        'R200405', \n",
    "        'R200506', \n",
    "        'R200607', \n",
    "        'R200708', \n",
    "        'R200809', \n",
    "        'R200910',\n",
    "        'R201011', \n",
    "        'R201112', \n",
    "        'R201213', \n",
    "        'R201314', \n",
    "        'R201415', \n",
    "        'R201516',\n",
    "        'R201617', \n",
    "        'R201718', \n",
    "        'R201819',\n",
    "        'V200001', \n",
    "        'V200102',\n",
    "        'V200203', \n",
    "        'V200304', \n",
    "        'V200405', \n",
    "        'V200506', \n",
    "        'V200607', \n",
    "        'V200708',\n",
    "        'V200809', \n",
    "        'V200910', \n",
    "        'V201011', \n",
    "        'V201112', \n",
    "        'V201213', \n",
    "        'V201314',\n",
    "        'V201415', \n",
    "        'V201516', \n",
    "        'V201617', \n",
    "        'V201718', \n",
    "        'V201819',\n",
    "        'E200001', \n",
    "        'E200102', \n",
    "        'E200203', \n",
    "        'E200304', \n",
    "        'E200405', \n",
    "        'E200506',\n",
    "        'E200607', \n",
    "        'E200708', \n",
    "        'E200809',\n",
    "        'E200910', \n",
    "        'E201011', \n",
    "        'E201112',\n",
    "        'E201213',\n",
    "        'E201314', \n",
    "        'E201415', \n",
    "        'E201516', \n",
    "        'E201617', \n",
    "        'E201718',\n",
    "        'E201819'}\n",
    "\n",
    "Individual_data_SUMMARY=data[0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Individual_data_SUMMARY.drop(to_drop, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The summary cleaning gets rid of the year-by-year data and leaves only the summary of the donations for analyses\n",
    "#obviously there are pros and cons to this, but I figured it was a tedious process to undertake so now it's a simple dictionary that can be used on the various data sets\n",
    "Individual_data_SUMMARY.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_data_SUMMARY=Final_data.copy()\n",
    "Final_data_SUMMARY.drop(to_drop, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_data_SUMMARY.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Psych 81.09",
   "language": "python",
   "name": "psych81.09"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
